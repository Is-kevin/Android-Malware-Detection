# 测试使用传统特征选择算法和传统降维算法的准确率
from sklearn.model_selection import KFold
from sklearn.ensemble.forest import RandomForestClassifier
from sklearn.neighbors.classification import KNeighborsClassifier
from sklearn import svm
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
import pandas as pd
import numpy as np
from sklearn import metrics
import warnings
# from se import SE
from tranditionalway.cs import CS
from tranditionalway.ig import IG
from tranditionalway.relief import Relief
# from eccd import ECCD
# from emsp import EMSP
# from adaboost import AdaBoost
# from rf import RF
# from dt import DT
# from lr import LR
import argparse

warnings.filterwarnings("ignore", category=FutureWarning, module="sklearn", lineno=196)

# 命令行参数
def parse_args():
    parser = argparse.ArgumentParser(description='testFS')
    # parser.add_argument('--method', choices=['IG', 'CS', 'Relief', 'SE', 'ECCD', 'EMSP','DT','LR'], required=True)
    parser.add_argument('--method', choices=['IG', 'CS', 'Relief', 'SE', 'AdaBoost', 'RF', 'DT', 'LR'], required=True)
    # parser.add_argument('--top', type=int, required=True)
    parser.add_argument('--top', type=int, choices=[100, 200, 500], required=True)
    parser.add_argument('--dataset', choices=['2000_3000', '2500_2500', '3000_2000'], required=True)
    return parser.parse_args()
    

# 计算评价指标
def get_result(label, prediction):
    accuracy = metrics.accuracy_score(label, prediction)
    #precision = metrics.precision_score(label, prediction)
    #recall = metrics.recall_score(label, prediction)
    f1 = metrics.f1_score(label, prediction)
    return accuracy, f1
    
        
if __name__ == '__main__':
    from datetime import datetime
    start = datetime.now()
    args = parse_args()
    method = args.method
    dataset = args.dataset
    topN = args.top
    feature_path = "dataset/feature" + dataset
    label_path = "dataset/label" + dataset

    feature = pd.read_csv(feature_path, sep=' ', header=None).values
    label = pd.read_csv(label_path, sep=' ', header=None).values.ravel()

    malicous_num = int(label.sum())
    benign_num = feature.shape[0] - malicous_num
    print("benign_num = " + str(benign_num))
    print("malicous_num = " + str(malicous_num))    
    
    kf = KFold(n_splits=5, shuffle=False)
    kf.get_n_splits(feature)

    knn_accuracy, knn_f1 = [], []
    rf_accuracy, rf_f1 = [], []
    svm_accuracy, svm_f1 = [], []
    
    accuracy = {}
    # accuracy['GNB'] = []
    # accuracy['MNB'] = []
    # accuracy['LDA'] = []
    # accuracy['QDA'] = []
    accuracy['SVM'] = []
    # accuracy['DT'] = []
    # accuracy['LR'] = []
    accuracy['KNN'] = []
    accuracy['RF'] = []
    # accuracy['MLPC'] = []

    f1 = {}
    f1['GNB'] = []
    f1['MNB'] = []
    f1['LDA'] = []
    f1['QDA'] = []
    f1['MLPC'] = []
    f1['SVM'] = []
    f1['DT'] = []
    f1['LR'] = []
    f1['KNN'] = []
    f1['RF'] = []
        
    current_folder = 0
    for train_index, test_index in kf.split(feature):
        print("current_folder = " + str(current_folder))
        current_folder = current_folder + 1
        
        fsm = None
        if method == "CS":
            fsm = CS(feature[train_index, :], label[train_index], topN)
        if method == "IG":
            fsm = IG(feature[train_index, :], label[train_index], topN)
        # if method == "SE":
            # fsm = SE(feature[train_index, :], label[train_index], topN)
        if method == "Relief":
            fsm = Relief(feature[train_index, :], label[train_index], topN)
        # if method == "RF":
        #     fsm = RF(feature[train_index, :], label[train_index], topN)
        # if method == "AdaBoost":
        #     fsm = AdaBoost(feature[train_index, :], label[train_index], topN)
        # if method == "DT":
        #     fsm = DT(feature[train_index, :], label[train_index], topN)
        # if method == "LR":
        #     fsm = LR(feature[train_index, :], label[train_index], topN)

        top = fsm.rank()

        # model = GaussianNB()
        # model.fit(feature[train_index,:][:,top], label[train_index])
        # prediction = model.predict(feature[test_index,:][:,top])
        # acc, f = get_result(label[test_index], prediction)
        # accuracy['GNB'].append(acc)
        # f1['GNB'].append(f)
        #
        # model = MultinomialNB()
        # model.fit(feature[train_index,:][:,top], label[train_index])
        # prediction = model.predict(feature[test_index,:][:,top])
        # acc, f = get_result(label[test_index], prediction)
        # accuracy['MNB'].append(acc)
        # f1['MNB'].append(f)
        
        # model = LinearDiscriminantAnalysis()
        # model.fit(feature[train_index,:][:,top], label[train_index])
        # prediction = model.predict(feature[test_index,:][:,top])
        # acc, f = get_result(label[test_index], prediction)
        # accuracy['LDA'].append(acc)
        # f1['LDA'].append(f)
        
        # model = QuadraticDiscriminantAnalysis()
        # model.fit(feature[train_index,:][:,top], label[train_index])
        # prediction = model.predict(feature[test_index,:][:,top])
        # acc, f = get_result(label[test_index], prediction)
        # accuracy['QDA'].append(acc)
        # f1['QDA'].append(f)
        
        model = svm.SVC(kernel='linear')
        model.fit(feature[train_index,:][:,top], label[train_index])
        prediction = model.predict(feature[test_index,:][:,top])
        acc, f = get_result(label[test_index], prediction)
        accuracy['SVM'].append(acc)
        f1['SVM'].append(f)
        
        # model = DecisionTreeClassifier()
        # model.fit(feature[train_index,:][:,top], label[train_index])
        # prediction = model.predict(feature[test_index,:][:,top])
        # acc, f = get_result(label[test_index], prediction)
        # accuracy['DT'].append(acc)
        # f1['DT'].append(f)
        
        # model = LogisticRegression()
        # model.fit(feature[train_index,:][:,top], label[train_index])
        # prediction = model.predict(feature[test_index,:][:,top])
        # acc, f = get_result(label[test_index], prediction)
        # accuracy['LR'].append(acc)
        # f1['LR'].append(f)
        
        model = KNeighborsClassifier(n_neighbors=1)
        model.fit(feature[train_index, :][:, top], label[train_index])
        prediction = model.predict(feature[test_index, :][:, top])
        acc, f = get_result(label[test_index], prediction)
        accuracy['KNN'].append(acc)
        f1['KNN'].append(f)

        model = RandomForestClassifier(n_estimators=250)
        model.fit(feature[train_index, :][:, top], label[train_index])
        prediction = model.predict(feature[test_index, :][:, top])
        acc, f = get_result(label[test_index], prediction)
        accuracy['RF'].append(acc)
        f1['RF'].append(f)

        # model = MLPClassifier()
        # model.fit(feature[train_index,:][:,top], label[train_index])
        # prediction = model.predict(feature[test_index,:][:,top])
        # acc, f = get_result(label[test_index], prediction)
        # accuracy['MLPC'].append(acc)
        # f1['MLPC'].append(f)
        
    for m in accuracy:
        print(m + ' accuracy: ' + str(round(np.array(accuracy[m]).mean(), 3)))
        print(m + ' f1: ' + str(round(np.array(f1[m]).mean(), 3)))

    end = (datetime.now() - start)
    print("总耗时："+str(end))
